{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0882c663",
   "metadata": {},
   "source": [
    "\n",
    "# Panoptic segmentation on NPM3D (Colab-ready)\n",
    "\n",
    "Ce notebook prépare l'environnement recommandé dans le README, télécharge les données NPM3D depuis Google Drive et montre comment lancer l'entraînement/l'évaluation tout en offrant la possibilité de brancher un algorithme de clustering personnalisé sur l'embedding 5D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a18079",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration rapide\n",
    "\n",
    "Utilisez les interrupteurs ci-dessous pour activer les étapes coûteuses. Par défaut, le notebook se contente d'initialiser les utilitaires et d'exécuter un test rapide du plugin de clustering de façon à pouvoir tourner même sans GPU. Activez les options lorsque vous travaillez sur une machine disposant de la configuration complète.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2761ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "if (repo_root / 'pyproject.toml').exists() and (repo_root / 'torch_points3d').exists():\n",
    "    pass\n",
    "elif (repo_root / 'PanopticSegForLargeScalePointCloud').exists():\n",
    "    repo_root = repo_root / 'PanopticSegForLargeScalePointCloud'\n",
    "    os.chdir(repo_root)\n",
    "else:\n",
    "    raise RuntimeError(\"Impossible de localiser la racine du dépôt. Exécutez le notebook depuis le répertoire du projet.\")\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "INSTALL_ENV = False\n",
    "DOWNLOAD_DATA = False\n",
    "RUN_TRAINING = False\n",
    "RUN_EVAL = False\n",
    "RUN_STATS = False\n",
    "USE_CUSTOM_CLUSTER = True\n",
    "\n",
    "ENV_NAME = 'treeins_env_local'\n",
    "CONDA_ROOT = Path('/opt/conda')\n",
    "\n",
    "DATA_ROOT = Path(os.environ.get('NPM3D_DATA_ROOT', repo_root / 'data'))\n",
    "OUTPUT_ROOT = Path(os.environ.get('NPM3D_OUTPUT_ROOT', repo_root / 'outputs'))\n",
    "CHECKPOINT_DIR = Path(os.environ['NPM3D_CHECKPOINT']) if os.environ.get('NPM3D_CHECKPOINT') else None\n",
    "EVAL_RESULTS_DIR = Path(os.environ['NPM3D_EVAL_DIR']) if os.environ.get('NPM3D_EVAL_DIR') else None\n",
    "\n",
    "CUSTOM_CLUSTER_SPEC = {\n",
    "    'target': 'scripts.custom_clustering.dbscan_plugin:cluster_embeddings',\n",
    "    'kwargs': {'eps': 0.45, 'min_samples': 15},\n",
    "    'default_type': 0,\n",
    "}\n",
    "\n",
    "JOB_NAME = f\"npm3d_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "env_flags = {\n",
    "    'INSTALL_ENV': '1' if INSTALL_ENV else '0',\n",
    "    'DOWNLOAD_DATA': '1' if DOWNLOAD_DATA else '0',\n",
    "    'RUN_TRAINING': '1' if RUN_TRAINING else '0',\n",
    "    'RUN_EVAL': '1' if RUN_EVAL else '0',\n",
    "    'RUN_STATS': '1' if RUN_STATS else '0',\n",
    "    'CONDA_ENV_NAME': ENV_NAME,\n",
    "    'CONDA_ROOT': str(CONDA_ROOT),\n",
    "    'DATA_ROOT': str(DATA_ROOT),\n",
    "    'OUTPUT_ROOT': str(OUTPUT_ROOT),\n",
    "}\n",
    "os.environ.update(env_flags)\n",
    "\n",
    "custom_cluster_overrides = []\n",
    "if USE_CUSTOM_CLUSTER and CUSTOM_CLUSTER_SPEC:\n",
    "    target = CUSTOM_CLUSTER_SPEC.get('target')\n",
    "    if target:\n",
    "        custom_cluster_overrides.append('models.PointGroup-PAPER.cluster_type=custom')\n",
    "        custom_cluster_overrides.append(f'models.PointGroup-PAPER.custom_clustering.target={target}')\n",
    "    for key, value in (CUSTOM_CLUSTER_SPEC.get('kwargs') or {}).items():\n",
    "        custom_cluster_overrides.append(\n",
    "            f'models.PointGroup-PAPER.custom_clustering.kwargs.{key}={value}'\n",
    "        )\n",
    "    if 'default_type' in CUSTOM_CLUSTER_SPEC:\n",
    "        custom_cluster_overrides.append(\n",
    "            f\"models.PointGroup-PAPER.custom_clustering.default_type={CUSTOM_CLUSTER_SPEC['default_type']}\"\n",
    "        )\n",
    "\n",
    "print('Interrupteurs :')\n",
    "flag_summary = {k: (v.lower() in {'1', 'true'}) for k, v in env_flags.items() if k in {'INSTALL_ENV', 'DOWNLOAD_DATA', 'RUN_TRAINING', 'RUN_EVAL', 'RUN_STATS'}}\n",
    "print(json.dumps(flag_summary, indent=2))\n",
    "print('Répertoires utilisés :')\n",
    "print(json.dumps({'data_root': str(DATA_ROOT), 'output_root': str(OUTPUT_ROOT)}, indent=2))\n",
    "print(f'Overrides clustering : {custom_cluster_overrides}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8722fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "def run_subprocess(cmd: Sequence[str], *, cwd=None, env=None, check=True):\n",
    "    final_env = os.environ.copy()\n",
    "    if env:\n",
    "        final_env.update(env)\n",
    "    printable = ' '.join(cmd)\n",
    "    print(f'$ {printable}')\n",
    "    subprocess.run(cmd, cwd=cwd, env=final_env, check=check)\n",
    "\n",
    "\n",
    "def conda_run_prefix():\n",
    "    conda_exe = CONDA_ROOT / 'bin' / 'conda'\n",
    "    if not conda_exe.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Conda introuvable dans {conda_exe}. Lancez l'installation de l'environnement au préalable.\"\n",
    "        )\n",
    "    return [str(conda_exe), 'run', '-n', ENV_NAME]\n",
    "\n",
    "\n",
    "print('Utilitaires initialisés.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91ada2",
   "metadata": {},
   "source": [
    "## Installation de l'environnement (optionnelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "if [[ \"${INSTALL_ENV:-0}\" != \"1\" ]]; then\n",
    "  echo \"Installation ignorée car INSTALL_ENV=0.\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "CONDA_ROOT=\"${CONDA_ROOT:-/opt/conda}\"\n",
    "ENV_NAME=\"${CONDA_ENV_NAME:-treeins_env_local}\"\n",
    "\n",
    "if [[ ! -d \"${CONDA_ROOT}\" ]]; then\n",
    "  echo \"Installation de Miniconda dans ${CONDA_ROOT}...\"\n",
    "  wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O Miniconda3.sh\n",
    "  bash Miniconda3.sh -b -p \"${CONDA_ROOT}\"\n",
    "else\n",
    "  echo \"Miniconda déjà présent dans ${CONDA_ROOT}.\"\n",
    "fi\n",
    "\n",
    "source \"${CONDA_ROOT}/etc/profile.d/conda.sh\"\n",
    "conda config --set always_yes yes\n",
    "conda update -n base conda\n",
    "conda deactivate >/dev/null 2>&1 || true\n",
    "\n",
    "if ! conda env list | grep -q \"^${ENV_NAME} \"; then\n",
    "  echo \"Création de l'environnement ${ENV_NAME}...\"\n",
    "  conda create -n \"${ENV_NAME}\" python=3.8\n",
    "else\n",
    "  echo \"L'environnement ${ENV_NAME} existe déjà.\"\n",
    "fi\n",
    "\n",
    "conda activate \"${ENV_NAME}\"\n",
    "conda install pytorch=1.9.0 torchvision=0.10.0 torchaudio=0.9.0 cudatoolkit=11.1 -c pytorch -c nvidia\n",
    "pip install numpy==1.19.5\n",
    "conda install openblas-devel -c anaconda\n",
    "export CUDA_HOME=/usr/local/cuda-11\n",
    "pip install -U git+https://github.com/NVIDIA/MinkowskiEngine -v --no-deps --install-option=\"--blas_include_dirs=${CONDA_PREFIX}/include\" --install-option=\"--blas=openblas\"\n",
    "pip install torch-scatter==2.0.8 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
    "pip install torch-sparse==0.6.12 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
    "pip install torch-geometric==1.7.2\n",
    "pip install -r requirements.txt\n",
    "pip install numba==0.55.1\n",
    "conda install -c conda-forge hdbscan==0.8.27\n",
    "conda install numpy-base==1.19.2\n",
    "pip install joblib==1.1.0\n",
    "conda deactivate\n",
    "echo \"Environnement ${ENV_NAME} prêt.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c363b8",
   "metadata": {},
   "source": [
    "\n",
    "## Téléchargement des données (optionnel)\n",
    "\n",
    "Les fichiers sont hébergés sur Google Drive. Le code suivant utilise l'API `gdown` depuis Python pour éviter les soucis de quoting dans les cellules bash. Si le téléchargement échoue (erreur 403, quota), montez votre Drive (`from google.colab import drive`) et copiez manuellement les fichiers dans `data/npm3dfused/raw`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de74521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if not DOWNLOAD_DATA:\n",
    "    print('Téléchargement ignoré car DOWNLOAD_DATA=0.')\n",
    "else:\n",
    "    data_root = Path(os.environ.get('DATA_ROOT', repo_root / 'data'))\n",
    "    raw_dir = data_root / 'npm3dfused' / 'raw'\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        import gdown  # type: ignore\n",
    "    except ModuleNotFoundError:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'gdown'], check=True)\n",
    "        import gdown  # type: ignore\n",
    "\n",
    "    folder_url = 'https://drive.google.com/drive/folders/1Tsb-sEFmjIk458RqqfIu9pq7PgSoAwbp?usp=sharing'\n",
    "    print(f'Téléchargement des fichiers NPM3D dans {raw_dir}')\n",
    "    gdown.download_folder(folder_url, quiet=False, use_cookies=False, output=str(raw_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36dc82",
   "metadata": {},
   "source": [
    "## Inspection du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_dir = Path(os.environ.get('DATA_ROOT', repo_root / 'data')) / 'npm3dfused' / 'raw'\n",
    "if raw_dir.exists():\n",
    "    train_files = sorted(raw_dir.glob('*_train.ply'))\n",
    "    val_files = sorted(raw_dir.glob('*_val.ply'))\n",
    "    test_files = sorted(raw_dir.glob('*_test.ply'))\n",
    "    print(f\"{len(train_files)} fichiers train / {len(val_files)} val / {len(test_files)} test trouvés dans {raw_dir}.\")\n",
    "else:\n",
    "    train_files = val_files = test_files = []\n",
    "    print(f'Pas de données trouvées dans {raw_dir}. Montez ou téléchargez le jeu de données.')\n",
    "TEST_FILES = test_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6087514",
   "metadata": {},
   "source": [
    "## Test du plugin de clustering personnalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_CUSTOM_CLUSTER and CUSTOM_CLUSTER_SPEC:\n",
    "    print('Chargement du plugin...')\n",
    "    try:\n",
    "        from torch_points3d.utils.custom_cluster import resolve_custom_clusterer\n",
    "        adapter = resolve_custom_clusterer(CUSTOM_CLUSTER_SPEC)\n",
    "    except ModuleNotFoundError as exc:\n",
    "        print(f\"Dépendances manquantes pour le plugin ({exc}). Installez l'environnement complet pour le tester.\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Échec du chargement du plugin: {exc}\")\n",
    "    else:\n",
    "        try:\n",
    "            import torch\n",
    "        except ModuleNotFoundError:\n",
    "            print('PyTorch non disponible dans ce runtime, test ignoré.')\n",
    "        else:\n",
    "            torch.manual_seed(0)\n",
    "            embeddings = torch.randn(256, 5)\n",
    "            embeddings[:128] += 2.0\n",
    "            embeddings[128:] -= 2.0\n",
    "            batch_ids = torch.zeros(256, dtype=torch.long)\n",
    "            global_indices = torch.arange(256, dtype=torch.long)\n",
    "            predicted_labels = torch.randint(0, 3, (256,), dtype=torch.long)\n",
    "            dummy_semantics = torch.randn(256, 9)\n",
    "            dummy_offsets = torch.zeros(256, 3)\n",
    "            clusters, cluster_types = adapter(\n",
    "                embeddings=embeddings,\n",
    "                batch_ids=batch_ids,\n",
    "                global_indices=global_indices,\n",
    "                predicted_labels=predicted_labels,\n",
    "                semantic_logits=dummy_semantics,\n",
    "                positions=torch.randn(256, 3),\n",
    "                offsets=dummy_offsets,\n",
    "                raw_positions=torch.randn(256, 3),\n",
    "                raw_embeddings=embeddings,\n",
    "                raw_offsets=dummy_offsets,\n",
    "                raw_semantic_logits=dummy_semantics,\n",
    "                raw_batch_ids=batch_ids,\n",
    "                ignore_labels=torch.tensor([-1, 0, 1], dtype=torch.long),\n",
    "                bandwidth=None,\n",
    "                metadata={'notebook': True},\n",
    "            )\n",
    "            print(f'Plugin OK : {len(clusters)} clusters (types {cluster_types.tolist()}).')\n",
    "else:\n",
    "    print('Plugin personnalisé désactivé.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0c6d2",
   "metadata": {},
   "source": [
    "## Commande d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_args = [\n",
    "    'python',\n",
    "    'train.py',\n",
    "    'task=panoptic',\n",
    "    'data=panoptic/npm3d-sparseconv_grid_012_R_16_cylinder_area1',\n",
    "    'models=panoptic/area4_ablation_3heads_5',\n",
    "    'model_name=PointGroup-PAPER',\n",
    "    'training=7_area1',\n",
    "    f'job_name={JOB_NAME}',\n",
    "]\n",
    "train_args.extend(custom_cluster_overrides)\n",
    "try:\n",
    "    conda_prefix_cmd = list(conda_run_prefix())\n",
    "except FileNotFoundError:\n",
    "    conda_prefix_cmd = None\n",
    "if conda_prefix_cmd:\n",
    "    display_command = ' '.join(conda_prefix_cmd + train_args)\n",
    "else:\n",
    "    display_command = ' '.join(train_args)\n",
    "print('Commande :')\n",
    "print(display_command)\n",
    "if RUN_TRAINING:\n",
    "    if not conda_prefix_cmd:\n",
    "        raise FileNotFoundError(\"Environnement conda introuvable. Lancez la cellule d'installation.\")\n",
    "    run_subprocess(conda_prefix_cmd + train_args, cwd=repo_root)\n",
    "else:\n",
    "    print(\"Entraînement ignoré (RUN_TRAINING=False).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6e53f",
   "metadata": {},
   "source": [
    "## Commande d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f814c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "eval_args = [\n",
    "    'python',\n",
    "    'eval.py',\n",
    "    'task=panoptic',\n",
    "    'data=panoptic/npm3d-sparseconv_grid_012_R_16_cylinder_area1',\n",
    "    'models=panoptic/area4_ablation_3heads_5',\n",
    "    'model_name=PointGroup-PAPER',\n",
    "    'training=7_area1',\n",
    "]\n",
    "if CHECKPOINT_DIR:\n",
    "    eval_args.append(f'checkpoint_dir={CHECKPOINT_DIR}')\n",
    "if TEST_FILES:\n",
    "    eval_args.append(f'data.fold={json.dumps([str(p) for p in TEST_FILES])}')\n",
    "eval_args.extend(custom_cluster_overrides)\n",
    "try:\n",
    "    conda_prefix_cmd = list(conda_run_prefix())\n",
    "except FileNotFoundError:\n",
    "    conda_prefix_cmd = None\n",
    "if conda_prefix_cmd:\n",
    "    display_command = ' '.join(conda_prefix_cmd + eval_args)\n",
    "else:\n",
    "    display_command = ' '.join(eval_args)\n",
    "print('Commande :')\n",
    "print(display_command)\n",
    "if RUN_EVAL:\n",
    "    if not CHECKPOINT_DIR:\n",
    "        raise ValueError(\"Définissez CHECKPOINT_DIR avant de lancer l'évaluation.\")\n",
    "    if not TEST_FILES:\n",
    "        raise ValueError(\"Aucun fichier test détecté. Téléchargez le jeu de données.\")\n",
    "    if not conda_prefix_cmd:\n",
    "        raise FileNotFoundError(\"Environnement conda introuvable. Lancez la cellule d'installation.\")\n",
    "    run_subprocess(conda_prefix_cmd + eval_args, cwd=repo_root)\n",
    "else:\n",
    "    print(\"Évaluation ignorée (RUN_EVAL=False).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f9604",
   "metadata": {},
   "source": [
    "## Statistiques finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72966ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN_STATS:\n",
    "    if not EVAL_RESULTS_DIR:\n",
    "        raise ValueError(\"Définissez EVAL_RESULTS_DIR vers le dossier des sorties d'évaluation.\")\n",
    "    try:\n",
    "        conda_prefix_cmd = list(conda_run_prefix())\n",
    "    except FileNotFoundError:\n",
    "        raise\n",
    "    run_subprocess(\n",
    "        conda_prefix_cmd + ['python', 'evaluation_stats_NPM3D.py'],\n",
    "        cwd=repo_root,\n",
    "        env={'TP3D_EVAL_RESULTS': str(EVAL_RESULTS_DIR)},\n",
    "        check=False,\n",
    "    )\n",
    "    print(\"Adaptez evaluation_stats_NPM3D.py pour utiliser TP3D_EVAL_RESULTS ou modifiez-le via argparse selon vos besoins.\")\n",
    "else:\n",
    "    print(\"Statistiques ignorées (RUN_STATS=False). Modifiez evaluation_stats_NPM3D.py ou utilisez vos propres scripts pour exploiter les sorties dans outputs/.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc96945",
   "metadata": {},
   "source": [
    "\n",
    "## Étapes suivantes\n",
    "\n",
    "1. Exécutez l'entraînement en activant `RUN_TRAINING`.\n",
    "2. Mettez à jour `CHECKPOINT_DIR` avec le dossier de sortie généré par l'entraînement, puis passez `RUN_EVAL` à `True`.\n",
    "3. Ajustez `evaluation_stats_NPM3D.py` pour pointer vers `EVAL_RESULTS_DIR` (ou adaptez le script pour lire l'argument d'environnement `TP3D_EVAL_RESULTS`) et activez `RUN_STATS` afin de comparer vos résultats à ceux de l'article.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}