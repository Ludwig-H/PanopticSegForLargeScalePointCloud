{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "NPM3D_PanopticSeg_embeddings_eval_micromamba.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NPM3D PanopticSeg — env stable (Py3.10) + embeddings + éval\nCe notebook évite le piège Python 3.12 / Torch 2.x en créant un **environnement Python 3.10** isolé via **micromamba**.\nOn installe les **versions récentes compatibles** avec MinkowskiEngine, puis on suit le pipeline officiel.\n\n**Stack choisie (stable et la plus récente compatible ME):**\n- Python 3.10 (environnement micromamba `npme`)\n- PyTorch **1.13.1 + cu116**\n- torch-geometric **2.3.1** (+ roues scatter/sparse/cluster/spline-conv pour 1.13.1+cu116)\n- MinkowskiEngine **0.5.4** (roues précompilées NVIDIA)\n\nTout ce qui tourne côté Python dans la suite passe par `micromamba run -n npme` pour être sûr qu’on reste dans le bon env."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Vérif GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n!nvidia-smi || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Installer micromamba, créer env Py3.10 et installer les libs compatibles ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n    %%bash\n    set -euxo pipefail\n\n    export MAMBA_ROOT_PREFIX=/content/micromamba\n    if [ ! -d \"$MAMBA_ROOT_PREFIX\" ]; then\n      curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n      install -Dm755 bin/micromamba /usr/local/bin/micromamba\n    fi\n\n    # Créer l'env si nécessaire\n    if ! micromamba env list | grep -q \"^npme\"; then\n      micromamba create -y -n npme -c conda-forge python=3.10 pip\n    fi\n\n    # Installer la stack dans l'env\n    micromamba run -n npme python -m pip install --upgrade pip wheel setuptools packaging\n\n    # Torch 1.13.1 + cu116\n    micromamba run -n npme python -m pip install --no-cache-dir \\\n      torch==1.13.1+cu116 torchvision==0.14.1+cu116 \\\n      -f https://download.pytorch.org/whl/torch_stable.html\n\n    # PyG wheels pour torch-1.13.1+cu116\n    micromamba run -n npme python -m pip install --no-cache-dir \\\n      --extra-index-url https://data.pyg.org/whl/torch-1.13.1+cu116.html \\\n      torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric==2.3.1\n\n    # MinkowskiEngine roues (évite toute compilation CUDA)\n    micromamba run -n npme python -m pip install --no-cache-dir \\\n      MinkowskiEngine==0.5.4 -f https://nvidia.github.io/MinkowskiEngine/\n\n    # Dépendances repo\n    micromamba run -n npme python -m pip install --no-cache-dir \\\n      hydra-core==1.1.0 omegaconf==2.1.0 plyfile==0.8.1 \\\n      scipy==1.10.1 hdbscan==0.8.29 pandas==1.5.3 numba==0.57.1 joblib==1.3.2 tqdm pyyaml\n\n    # Sanity check\n    micromamba run -n npme python - <<'PY'\nimport torch, sys\nprint(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda, \"CUDA available:\", torch.cuda.is_available())\ntry:\n    import MinkowskiEngine as ME\n    print(\"MinkowskiEngine:\", ME.__version__)\nexcept Exception as e:\n    print(\"ME import failed:\", e); sys.exit(1)\ntry:\n    import torch_geometric\n    print(\"torch_geometric:\", torch_geometric.__version__)\nexcept Exception as e:\n    print(\"pyg import failed:\", e); sys.exit(1)\nPY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Cloner ton fork du repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n%%bash\nset -euxo pipefail\n\nREPO_URL=\"https://github.com/Ludwig-H/PanopticSegForLargeScalePointCloud\"\nif [ ! -d PanopticSegForLargeScalePointCloud ]; then\n  git clone --depth=1 \"$REPO_URL\"\nfi\ncd PanopticSegForLargeScalePointCloud\nmkdir -p data/npm3dfused/raw outputs\necho \"Repo ok.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Télécharger NPM3D (labels d’instances) dans `data/npm3dfused/raw`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n%%bash\nset -euxo pipefail\ncd PanopticSegForLargeScalePointCloud/data/npm3dfused/raw\n\nbase=\"https://zenodo.org/records/8118986/files\"\nfiles=(\n  \"Paris_train.ply\" \"Paris_val.ply\" \"Paris_test.ply\"\n  \"Lille1_1_train.ply\" \"Lille1_1_val.ply\" \"Lille1_1_test.ply\"\n  \"Lille1_2_train.ply\" \"Lille1_2_val.ply\" \"Lille1_2_test.ply\"\n  \"Lille2_train.ply\" \"Lille2_val.ply\" \"Lille2_test.ply\"\n)\nfor f in \"${files[@]}\"; do\n  if [ ! -f \"$f\" ]; then\n    echo \"Downloading $f\"\n    wget -q --show-progress \"${base}/${f}?download=1\" -O \"$f\"\n  else\n    echo \"Already have $f\"\n  fi\ndone\nls -lh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) (Optionnel) Entraînement Setting IV (rayon 16 m, voxel 0.12)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n%%bash\nset -euxo pipefail\ncd PanopticSegForLargeScalePointCloud\nexport WANDB_MODE=offline\n\necho \"Exemple de commande (désactivée par défaut) :\"\necho \"micromamba run -n npme python train.py task=panoptic \\\\\n  data=panoptic/npm3d-sparseconv_grid_012_R_16_cylinder_area1 \\\\\n  models=panoptic/area4_ablation_3heads_5 model_name=PointGroup-PAPER \\\\\n  training=7_area1 job_name=A1_S7\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Adapter `conf/eval.yaml` et préparer un checkpoint\nSi tu n’entraînes pas dans cette session, monte ton Drive et pointe `CKPT_PATH` sur un checkpoint `best.pth` existant.\nLe fichier `conf/eval.yaml` contrôle chemins et split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n%%bash\nset -euxo pipefail\ncd PanopticSegForLargeScalePointCloud\necho \"Aperçu conf/eval.yaml (éditable si besoin):\"\nsed -n '1,200p' conf/eval.yaml || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Extraction **embeddings 5D** avant clustering\nOn ajoute un script indépendant qui charge le modèle/dataloader d’éval via Hydra, accroche un hook sur la tête d’embedding et dump des `.npz`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n    %%bash\n    set -euxo pipefail\n    cd PanopticSegForLargeScalePointCloud\n\n    cat > tools_extract_embeddings.py << 'PY'\nimport os, re, numpy as np, torch\nfrom pathlib import Path\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\n# Stratégie: réutiliser la construction des loaders & du modèle d'eval.\n# Le repo expose généralement des helpers dans eval.py; sinon, on importe train/eval et\n# cherche les objets dans le namespace hydra.\n\ndef _attach_first_embed_hook(model, bucket):\n    chosen = None\n    for name, m in model.named_modules():\n        if re.search(r\"(emb|embed|embedding)\", name, re.I):\n            chosen = (name, m)\n            break\n    if chosen is None:\n        print(\"[extract] Aucun module 'emb*' trouvé; on essaiera de lire la sortie du forward dict.\")\n        return None\n    name, mod = chosen\n    print(f\"[extract] Hook sur '{name}'\")\n    def _hook(module, inp, out):\n        try:\n            E = out.detach().float().cpu().numpy()\n            bucket.append(E)\n        except Exception as e:\n            print(\"[extract] hook fail:\", e)\n    h = mod.register_forward_hook(lambda m,i,o: _hook(m,i,o))\n    return h\n\n@hydra.main(config_path=\"conf\", config_name=\"eval\", version_base=None)\ndef main(cfg: DictConfig):\n    # On s'appuie sur les mêmes builders que eval.py\n    # Les fonctions peuvent s'appeler différemment selon commit; on essaye plusieurs options.\n    build_ok = False\n    model = None; loaders = None; device = None; test_split = None\n    try:\n        from eval import build_model_and_loaders\n        model, loaders, device, test_split = build_model_and_loaders(cfg)\n        build_ok = True\n        print(\"[extract] build_model_and_loaders OK\")\n    except Exception as e:\n        print(\"[extract] Pas de build_model_and_loaders dans eval.py:\", e)\n\n    if not build_ok:\n        try:\n            # Certains repos exposent make_model, make_dataloaders\n            from eval import make_model, make_dataloaders, get_device\n            device = get_device(cfg)\n            model = make_model(cfg).to(device).eval()\n            loaders, test_split = make_dataloaders(cfg)\n            build_ok = True\n            print(\"[extract] make_model/make_dataloaders OK\")\n        except Exception as e:\n            print(\"[extract] Echec construction modèle/loaders:\", e)\n            raise SystemExit(1)\n\n    out_root = Path(\"outputs/embeddings\")\n    out_root.mkdir(parents=True, exist_ok=True)\n\n    bucket = []\n    hook = _attach_first_embed_hook(model, bucket)\n\n    test_loader = loaders.get(test_split, None) or loaders.get(\"test\", None)\n    if test_loader is None:\n        print(\"[extract] test_loader introuvable. Clés:\", list(loaders.keys()))\n        raise SystemExit(1)\n\n    with torch.no_grad():\n        for ib, batch in enumerate(test_loader):\n            batch = batch.to(device)\n            out = model(batch)\n            # Si le modèle retourne un dict avec une clé embeddings*\n            if isinstance(out, dict):\n                for k in [\"embedding\", \"embeddings\", \"embedding_5d\", \"emb_5d\", \"panoptic_embeddings\"]:\n                    if k in out and isinstance(out[k], torch.Tensor):\n                        E = out[k].detach().cpu().float().numpy()\n                        bucket.append(E)\n                        break\n            if not bucket:\n                print(f\"[extract] WARNING: pas d'embeddings capturés pour batch {ib}.\")\n                continue\n            E = bucket[-1]\n            idx = getattr(batch, \"idx\", None)\n            if idx is None and hasattr(batch, \"ptr\"):\n                idx = batch.ptr\n            if idx is None:\n                idx = torch.arange(E.shape[0], device=E.device)\n            idx = idx.detach().cpu().numpy()\n            tag = f\"batch_{ib:05d}\"\n            np.savez_compressed(out_root / f\"{tag}.npz\", embeddings=E, indices=idx)\n            print(f\"[extract] saved {tag}: {E.shape}\")\n    if hook is not None:\n        hook.remove()\n\nif __name__ == \"__main__\":\n    main()\nPY\n\n    echo \"tools_extract_embeddings.py créé.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Lancer éval officielle puis extraction d’embeddings\nAdapte `CKPT_PATH` si besoin. Tu peux aussi surcharger la conf Hydra en CLI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n%%bash\nset -euxo pipefail\ncd PanopticSegForLargeScalePointCloud\n\n# Exemple: pointer vers un checkpoint existant (modifie ce chemin selon ton stockage)\nexport CKPT_PATH=\"${CKPT_PATH:-/content/drive/MyDrive/panoptic_runs/A1_S7/checkpoints/best.pth}\"\n\necho \"Si nécessaire, lance l'éval (désactivée ici si conf déjà prête):\"\necho \"micromamba run -n npme python eval.py\"\n\necho \"Extraction embeddings:\"\nmicromamba run -n npme python tools_extract_embeddings.py || true\nls -lh outputs/embeddings || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Brancher ton clusterer sur les embeddings `.npz`\nCette cellule de **démonstration** lit tous les `.npz`, normalise, applique un clusterer bidon et sauve un `instance_labels.npy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n    %%bash\n    set -euxo pipefail\n    cd PanopticSegForLargeScalePointCloud\n\n    cat > tools_cluster_with_embeddings.py << 'PY'\nimport os, glob, numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nemb_dir = \"outputs/embeddings\"\nout_path = \"outputs/instance_labels.npy\"\n\ndef load_all_npz(d):\n    Xs, Is = [], []\n    for f in sorted(glob.glob(os.path.join(d, \"*.npz\"))):\n        d = np.load(f)\n        Xs.append(d[\"embeddings\"])\n        Is.append(d[\"indices\"])\n    X = np.concatenate(Xs, axis=0) if Xs else np.zeros((0,5), dtype=np.float32)\n    I = np.concatenate(Is, axis=0) if Is else np.zeros((0,), dtype=np.int64)\n    return X, I\n\ndef my_clusterer(X):\n    # TODO: remplace par ton algo\n    return np.zeros(X.shape[0], dtype=np.int32)\n\nX, I = load_all_npz(emb_dir)\nif X.shape[0] == 0:\n    raise SystemExit(\"Aucun embedding trouvé. Vérifie l'étape précédente.\")\nXn = StandardScaler().fit_transform(X)\nlabels = my_clusterer(Xn)\nnp.save(out_path, labels)\nprint(\"Saved:\", out_path, labels.shape)\nPY\n\n    micromamba run -n npme python -m pip install scikit-learn || true\n    micromamba run -n npme python tools_cluster_with_embeddings.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Évaluation finale (scripts officiels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n%%bash\nset -euxo pipefail\ncd PanopticSegForLargeScalePointCloud\nmicromamba run -n npme python evaluation_stats_NPM3D.py || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n- Si tu tiens absolument à rester en Python 3.12/Torch 2.x natif, **MinkowskiEngine** risque de ne pas avoir de roue binaire. Ne compile pas en Colab, c’est peine perdue sans toolkit CUDA complet. L’env micromamba **résout le problème proprement**.\n- Tu peux dupliquer l’éval sur `area{1..4}` (4-fold) en adaptant la conf Hydra comme dans le README.\n- Si la capture d’embeddings ne prend pas, imprime `model.named_modules()` et ajuste le motif de recherche `'emb'` dans `tools_extract_embeddings.py`."
      ]
    }
  ]
}